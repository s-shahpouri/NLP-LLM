# Running Locally

To run this project locally, you'll need to have an Ollama instance set up with the LLaMA 2 model installed. Alternatively, you can download any other model of your choice from [Ollama.com](https://ollama.com).

Before we dive in, please note that Ollama provides a reference guide on how to use their models with OpenAI libraries. You can find more information on this link: https://ollama.com/blog/openai-compatibility

**Step-by-Step Instructions**

1. Ensure you have Python installed on your machine.
2. Install the necessary dependencies by running `pip install -r requirements.txt` in your terminal or command prompt.
3. Start your Ollama instance and ensure the LLaMA 2 model is installed.
4. Run the application using Python: `python app.py`
5. Interact with the console-based application as you would with any other Python script.

That's it! You should now be able to run this project locally and start exploring the power of Ollama models in your own projects.
