{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LLM (Large Language Models)\n",
    "\n",
    "Large Language Models (LLM) are advanced neural network architectures designed to process and generate human language. They are trained on extensive text datasets, enabling them to understand context, syntax, and semantics.\n",
    "\n",
    "The primary function of LLMs is to predict the next word in a sequence based on prior context. This capability allows them to perform various tasks, including:\n",
    "\n",
    "* Text Generation: Creating coherent and contextually relevant content.\n",
    "* Translation: Converting text from one language to another while maintaining meaning.\n",
    "* Sentiment Analysis: Determining the emotional tone behind a piece of text.\n",
    "* Conversational Agents: Powering chatbots and virtual assistants for interactive dialogues.\n",
    "\n",
    "Training LLMs involves two phases: pre-training on vast datasets to learn general language patterns, followed by fine-tuning on specific tasks to enhance performance. While LLMs have transformed NLP, they also raise concerns about bias and ethical use, prompting ongoing research in these areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is LLM\n",
    "LLM can be described as having two main components:\n",
    "\n",
    "A large weights file. This file stores the billions of parameters that the model uses to predict text. For example, the Llama-2-70B model contains 70 billion parameters and weighs in at about 140 GB.\n",
    "The code to run the model. A small script of about 500 lines of code that is responsible for interacting with the model and making its predictions.\n",
    "Although the model takes up a significant amount of disk space, it can be run even on common devices like a MacBook without the need for specialized servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stages of LLM creation\n",
    "## Stage 1: Pretraining\n",
    "Pretraining is the process of training the model on huge volumes of text. This stage can be thought of as the process of compressing the Internet into a neural network.\n",
    "\n",
    "Key steps of pretraining:\n",
    "\n",
    "1. The model is trained on a huge volume of text data (~10 terabytes of text).\n",
    "2. Powerful computing clusters with thousands of graphics processing units (GPUs) are used for training.\n",
    "3. The pretraining process can take several weeks and cost millions of dollars.\n",
    "\n",
    "**Example:** The Llama-2 model was trained on texts from the Internet and is able to predict the next word based on the previous ones. For the phrase \"The cat sat on a\", the model will predict the word \"mat\" with a high probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T05:40:10.151641Z",
     "iopub.status.busy": "2024-09-21T05:40:10.151097Z",
     "iopub.status.idle": "2024-09-21T05:40:11.369325Z",
     "shell.execute_reply": "2024-09-21T05:40:11.368075Z",
     "shell.execute_reply.started": "2024-09-21T05:40:10.151593Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The cat sat on a\n",
      "Predicted text: The cat sat on a bench, and the cat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "input_text = \"The cat sat on a\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    #outputs = model.generate(inputs[\"input_ids\"], max_length=10)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=10, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Original text:\", input_text)\n",
    "print(\"Predicted text:\", predicted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Finetuning\n",
    "After pre-training, the model is not very useful in practical tasks. To make it useful, fine-tuning is carried out on specialized datasets created manually.\n",
    "\n",
    "**Goal:** Turn the basic model into an assistant that can answer questions and solve user problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Example: Code for Text Translation\n",
    "Now let's look at some code examples for using a large language model (LLM) to translate text. We'll use a transformer-based model (like Llama-2) and the transformers library from Hugging Face.\n",
    "\n",
    "## Loading the Model\n",
    "Next, we will load the pre-trained translation model. For this example, we'll use a hypothetical translation model from Hugging Face's model hub:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T05:41:27.784787Z",
     "iopub.status.busy": "2024-09-21T05:41:27.784185Z",
     "iopub.status.idle": "2024-09-21T05:41:32.817504Z",
     "shell.execute_reply": "2024-09-21T05:41:32.815827Z",
     "shell.execute_reply.started": "2024-09-21T05:41:27.784732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\zshahpouri\\nlp_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cf037bd6a64e95afa37fc4c7da61c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\zshahpouri\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zshah\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-es. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49740258bc264c45b993f7b5a442e94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/826k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73abc7e9ac54941b9f62297ec406812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\zshahpouri\\nlp_env\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f5c39502194b23bdc7654e103000fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\zshahpouri\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zshah\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-es-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c20d04401346f48c40939a914fde6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be8fd3061ec40198b0a3eb55fc94161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7360cb0a80214bd2915d5b16b8ebd6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84265b9f9dde410fb8633ba56c151438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/826k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f961eceeaf1041f488bff583adcc6169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241abdc91292451faa84272fa1a8f517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator_en_to_es = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "translator_es_to_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating Text\n",
    "Now we can translate text from one language to another. Let's create a function to handle translations between English and Spanish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T05:41:37.872768Z",
     "iopub.status.busy": "2024-09-21T05:41:37.871608Z",
     "iopub.status.idle": "2024-09-21T05:41:39.18106Z",
     "shell.execute_reply": "2024-09-21T05:41:39.179709Z",
     "shell.execute_reply.started": "2024-09-21T05:41:37.872713Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated from English to Spanish: 'Hello, how are you?' -> 'Hola, ¿cómo estás?'\n",
      "Translated from English to Spanish: 'What is your name?' -> '¿Cómo te llamas?'\n",
      "Translated from English to Spanish: 'I love learning new languages.' -> 'Me encanta aprender nuevos idiomas.'\n",
      "Translated from Spanish to English: 'La vida es un sueño.' -> 'Life is a dream.'\n",
      "Translated from Spanish to English: '¿Dónde está la biblioteca?' -> 'Where's the library?'\n"
     ]
    }
   ],
   "source": [
    "def translate_text(text, source_lang='en', target_lang='es'):\n",
    "    if source_lang == 'en' and target_lang == 'es':\n",
    "        translated = translator_en_to_es(text)\n",
    "    elif source_lang == 'es' and target_lang == 'en':\n",
    "        translated = translator_es_to_en(text)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language pair\")\n",
    "\n",
    "    return translated[0]['translation_text']\n",
    "\n",
    "texts_to_translate = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"I love learning new languages.\",\n",
    "    \"La vida es un sueño.\",\n",
    "    \"¿Dónde está la biblioteca?\"\n",
    "]\n",
    "\n",
    "for text in texts_to_translate:\n",
    "    if text.isascii(): \n",
    "        translated_text = translate_text(text, source_lang='en', target_lang='es')\n",
    "        print(f\"Translated from English to Spanish: '{text}' -> '{translated_text}'\")\n",
    "    else:\n",
    "        translated_text = translate_text(text, source_lang='es', target_lang='en')\n",
    "        print(f\"Translated from Spanish to English: '{text}' -> '{translated_text}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Multiple Translations\n",
    "You might want to translate a list of sentences efficiently. Here’s an enhanced version that handles multiple translations in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T05:41:44.569344Z",
     "iopub.status.busy": "2024-09-21T05:41:44.568916Z",
     "iopub.status.idle": "2024-09-21T05:41:45.933083Z",
     "shell.execute_reply": "2024-09-21T05:41:45.931799Z",
     "shell.execute_reply.started": "2024-09-21T05:41:44.569302Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6f7b52d48e4c94a93d85eb04e762c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'Hello, how are you?' -> Translated: 'Hola, ¿cómo estás?'\n",
      "Original: 'What is your name?' -> Translated: '¿Cómo te llamas?'\n",
      "Original: 'I love learning new languages.' -> Translated: 'Me encanta aprender nuevos idiomas.'\n",
      "Original: 'La vida es un sueño.' -> Translated: 'La vida es un sueño.'\n",
      "Original: '¿Dónde está la biblioteca?' -> Translated: '¿Dónde está la biblioteca?'\n"
     ]
    }
   ],
   "source": [
    "def translate_multiple_texts(texts, source_lang='en', target_lang='es'):\n",
    "    translations = []\n",
    "    for text in texts:\n",
    "        translated = translate_text(text, source_lang, target_lang)\n",
    "        translations.append(translated)\n",
    "    return translations\n",
    "\n",
    "texts_to_translate = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"I love learning new languages.\",\n",
    "    \"La vida es un sueño.\",\n",
    "    \"¿Dónde está la biblioteca?\"\n",
    "]\n",
    "\n",
    "translations = translate_multiple_texts(texts_to_translate)\n",
    "for original, translated in zip(texts_to_translate, translations):\n",
    "    print(f\"Original: '{original}' -> Translated: '{translated}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "Now let's look at an example of how to use LLM to generate text from a given start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T05:41:48.691556Z",
     "iopub.status.busy": "2024-09-21T05:41:48.690099Z",
     "iopub.status.idle": "2024-09-21T05:41:53.111107Z",
     "shell.execute_reply": "2024-09-21T05:41:53.109907Z",
     "shell.execute_reply.started": "2024-09-21T05:41:48.691501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Once upon a time in a land far away, the sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt_text = \"Once upon a time in a land far away,\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "We use LLM to analyze sentiment in text. In this example, we will apply the model to classify text as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T05:42:22.747734Z",
     "iopub.status.busy": "2024-09-21T05:42:22.747232Z",
     "iopub.status.idle": "2024-09-21T05:42:23.145481Z",
     "shell.execute_reply": "2024-09-21T05:42:23.143898Z",
     "shell.execute_reply.started": "2024-09-21T05:42:22.747692Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [{'label': 'POSITIVE', 'score': 0.9998843669891357}]\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "text_to_analyze = \"I love using this new model! It's amazing.\"\n",
    "\n",
    "sentiment_result = sentiment_pipeline(text_to_analyze)\n",
    "print(\"Result:\", sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "LLMs are a powerful tool for natural language processing, allowing us to perform many tasks such as translation, text generation, and sentiment analysis. With their help, we can significantly simplify various processes related to working with text information. However, their creation requires enormous computing power and data. At the same time, as technology develops, we can expect the emergence of models that will be increasingly adapted to solving specific problems, and their capabilities will only expand."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
