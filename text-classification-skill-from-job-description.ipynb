{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ikig5Lx2PgJT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB,ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from langdetect import detect\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work With 'Job Description'\n",
    "Explore data and select feature to modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m jobpostDF \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/zshahpouri/data/postings/postings.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m jobpostDF\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\zshahpouri\\nlp_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\zshahpouri\\nlp_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\zshahpouri\\nlp_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\zshahpouri\\nlp_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\zshahpouri\\nlp_env\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 1"
     ]
    }
   ],
   "source": [
    "jobpostDF = pd.read_csv('C:/zshahpouri/data/postings/postings.csv')\n",
    "jobpostDF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uN1V8Lo4aPa"
   },
   "outputs": [],
   "source": [
    "jobpostDF.isnull().sum()\n",
    "jobpostDF = jobpostDF.dropna(subset='description')\n",
    "jobpostDF = jobpostDF.loc[:,['job_id','title','description']]\n",
    "jobpostDF.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Text Cleaner with Various Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pps2zjNh8Kj_"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"u.s\": \"america\",\n",
    "    \"e.g\": \"for example\",\n",
    "}\n",
    "\n",
    "\n",
    "# Clean contraction\n",
    "def clean_contractions(text):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    for word in contraction_mapping.keys():\n",
    "        if \"\" + word + \"\" in text:\n",
    "            text = text.replace(\"\" + word + \"\", \"\" + contraction_mapping[word] + \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "# Remove Url Pattern\n",
    "def remove_urls(text):\n",
    "    url_pattern = r\"https?://\\S+|www\\.\\S+\"\n",
    "    return re.sub(url_pattern, \"\", text)\n",
    "\n",
    "\n",
    "# Remove HTML Tag\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile(\"<.*?>\")\n",
    "    return html_pattern.sub(r\"\", text)\n",
    "\n",
    "\n",
    "# Remove special character \"!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\"\n",
    "def remove_punctuation(text):\n",
    "    punctuation = string.punctuation + \"–\"\n",
    "    return re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)\n",
    "\n",
    "\n",
    "# Remove E-mail pattern\n",
    "def remove_emails(text):\n",
    "    return re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "\n",
    "\n",
    "# Remove New Line Code Snippet\n",
    "def remove_code_snippet(text):\n",
    "    return text.replace(\"\\n\", \"\")\n",
    "\n",
    "\n",
    "# Remove Emoji\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r\"\", string)\n",
    "\n",
    "\n",
    "# Remove Non-English Text\n",
    "def remove_non_english_text(text):\n",
    "    def is_english(text):\n",
    "        try:\n",
    "            return detect(text) == \"en\"\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    return text if is_english(text) else \"\"\n",
    "\n",
    "\n",
    "# Remove Digits\n",
    "def remove_digits(text):\n",
    "    return \"\".join(filter(lambda char: not char.isdigit(), text))\n",
    "\n",
    "\n",
    "# Remove Stop Words\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    custom_stop_words = [\n",
    "        \"job\",\n",
    "        \"role\",\n",
    "        \"position\",\n",
    "        \"responsibility\",\n",
    "        \"responsibilities\",\n",
    "        \"duties\",\n",
    "        \"duty\",\n",
    "        \"requirement\",\n",
    "        \"requirements\",\n",
    "        \"qualification\",\n",
    "        \"qualifications\",\n",
    "        \"description\",\n",
    "        \"descriptions\",\n",
    "        \"candidate\",\n",
    "        \"candidates\",\n",
    "        \"applicant\",\n",
    "        \"applicants\",\n",
    "        \"opportunity\",\n",
    "        \"opportunities\",\n",
    "        \"team\",\n",
    "        \"teams\",\n",
    "        \"work\",\n",
    "        \"working\",\n",
    "        \"employee\",\n",
    "        \"employees\",\n",
    "        \"employer\",\n",
    "        \"employers\",\n",
    "        \"company\",\n",
    "        \"companies\",\n",
    "        \"location\",\n",
    "        \"locations\",\n",
    "        \"department\",\n",
    "        \"departments\",\n",
    "        \"report\",\n",
    "        \"reports\",\n",
    "        \"reporting\",\n",
    "        \"benefit\",\n",
    "        \"benefits\",\n",
    "        \"compensation\",\n",
    "        \"salary\",\n",
    "        \"experience\",\n",
    "        \"experienced\",\n",
    "        \"year\",\n",
    "        \"years\",\n",
    "        \"gender\",\n",
    "        \"race\",\n",
    "        \"color\",\n",
    "        \"sex\",\n",
    "        \"orientation\",\n",
    "        \"sexual\",\n",
    "        \"religion\",\n",
    "        \"national\",\n",
    "        \"identify\",\n",
    "        \"veteran\",\n",
    "        \"nation\",\n",
    "        \"including\",\n",
    "        \"required\",\n",
    "        \"disability\",\n",
    "        \"regard\"\n",
    "    ]\n",
    "\n",
    "    words = text.split()\n",
    "    filtered_words = [\n",
    "        word\n",
    "        for word in words\n",
    "        if word.lower() not in stop_words and word.lower() not in custom_stop_words\n",
    "    ]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "# Lemmatization\n",
    "def lemmatize_words(text):\n",
    "    words = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text,remove_stop_words=True):\n",
    "    text = text.lower()\n",
    "    text = clean_contractions(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_code_snippet(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_emails(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_non_english_text(text)\n",
    "    text = remove_digits(text)\n",
    "    text = lemmatize_words(text)\n",
    "    if remove_stop_words:\n",
    "        text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Clean 'Job Description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Job description text\n",
    "jobpostDF['description_cleaned'] = jobpostDF['description'].astype(str).apply(lambda x: clean_text(x))\n",
    "jobpostDF['description_cleaned_st'] = jobpostDF['description'].astype(str).apply(lambda x: clean_text(x, remove_stop_words=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect **most common** word that can occur in every job description and also collect **rare word** then remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "for text in jobpostDF[\"description_cleaned\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "freqWords = set([w[0] for w in cnt.most_common(10)])\n",
    "rareWords = set([w for w, freq in cnt.items() if freq == 1])\n",
    "\n",
    "def remove_freq_rare_words(text):\n",
    "    preserved_words = {'management', 'product', 'project'}\n",
    "    return \" \".join([word for word in str(text).split() if word not in freqWords and word not in rareWords or word in preserved_words])\n",
    "jobpostDF['description_cleaned'] = jobpostDF['description_cleaned'].apply(remove_freq_rare_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count 'Job Description' Length and Remove text that have 0,1,2 length because it not make sense and can be empty text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Text Lenght\n",
    "#Remove Description that have 0 and 1 length\n",
    "jobpostDF['original_length'] = jobpostDF['description'].str.split().apply(len)\n",
    "jobpostDF['cleaned_length'] = jobpostDF['description_cleaned'].str.split().apply(len)\n",
    "jobpostDF = jobpostDF[jobpostDF['cleaned_length'].isin([0,1,2])==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Job Description Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of Text Length for Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,7))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "sns.histplot(jobpostDF['original_length'], ax=ax1, color='blue', bins=30, zorder=1)\n",
    "ax1.set_title('Original Descriptions')\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "sns.histplot(jobpostDF['cleaned_length'], ax=ax2, color='green', bins=30, zorder=1)\n",
    "ax2.set_title('Cleaned Descriptions')\n",
    "\n",
    "\n",
    "describe_original = jobpostDF.original_length.describe().to_frame().round(2)\n",
    "bbox_original = [0.65, 0.55, 0.3, 0.4]\n",
    "table_original = ax1.table(cellText=describe_original.values, rowLabels=describe_original.index, bbox=bbox_original, colLabels=describe_original.columns, zorder=2)\n",
    "table_original.auto_set_font_size(False)\n",
    "table_original.set_fontsize(12)\n",
    "# table_original.auto_set_column_width(col=list(range(len(describe_original.columns))))\n",
    "for key, cell in table_original.get_celld().items():\n",
    "    cell.set_text_props(ha='center', va='center')\n",
    "    cell.set_height(0.2) \n",
    "\n",
    "\n",
    "describe_cleaned = jobpostDF.cleaned_length.describe().to_frame().round(2)\n",
    "bbox_cleaned = [0.65, 0.55, 0.3, 0.4]\n",
    "table_cleaned = ax2.table(cellText=describe_cleaned.values, rowLabels=describe_cleaned.index, bbox=bbox_cleaned, colLabels=describe_cleaned.columns, zorder=2)\n",
    "table_cleaned.auto_set_font_size(False)\n",
    "table_cleaned.set_fontsize(12)\n",
    "# table_cleaned.auto_set_column_width(col=list(range(len(describe_cleaned.columns))))\n",
    "for key, cell in table_cleaned.get_celld().items():\n",
    "    cell.set_text_props(ha='center', va='center')\n",
    "    cell.set_height(0.2) \n",
    "    \n",
    "fig.suptitle('Distribution of Text Length for Job Description: Before vs. After Cleaning', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Clound Before and After Clean Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclean = ' '.join([text for text in jobpostDF['description']])\n",
    "clean = ' '.join([text for text in jobpostDF['description_cleaned']])\n",
    "\n",
    "# Generate word clouds\n",
    "wordcloud_unclean = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(unclean)\n",
    "wordcloud_clean = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(clean)\n",
    "\n",
    "plt.figure(figsize=(24, 9))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud_unclean, interpolation=\"bilinear\")\n",
    "plt.title(\"Unclean Text\", fontsize = 20)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloud_clean, interpolation=\"bilinear\")\n",
    "plt.title(\"Cleaned Text\", fontsize = 20)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 10 Word in Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "for text in jobpostDF['description_cleaned'].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "\n",
    "mostCommon = cnt.most_common(10)\n",
    "\n",
    "words = []\n",
    "freq = []\n",
    "for word, count in mostCommon:\n",
    "    words.append(word)\n",
    "    freq.append(count)\n",
    "\n",
    "sns.barplot(x=freq, y=words)\n",
    "plt.title('Top 10 Most Frequently Occuring Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Unigrams , Bigrams and Trigrams in 'Job Description' before and after remove 'Stop Word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_ngrams(corpus, ngram_range, n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    \n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    common_words = words_freq[:n]\n",
    "    words = []\n",
    "    freqs = []\n",
    "    for word, freq in common_words:\n",
    "        words.append(word)\n",
    "        freqs.append(freq)\n",
    "        \n",
    "    df = pd.DataFrame({'Word': words, 'Freq': freqs})\n",
    "    return df\n",
    "#collect n-gram for job description without Stop Word\n",
    "unigrams = get_top_ngrams(jobpostDF['description_cleaned'], (1, 1),20)\n",
    "bigrams = get_top_ngrams(jobpostDF['description_cleaned'], (2, 2), 20)\n",
    "trigrams = get_top_ngrams(jobpostDF['description_cleaned'], (3, 3),20)\n",
    "#collect n-gram for job description with Stop Word\n",
    "unigrams_st = get_top_ngrams(jobpostDF['description_cleaned_st'], (1, 1),20)\n",
    "bigrams_st = get_top_ngrams(jobpostDF['description_cleaned_st'], (2, 2),20)\n",
    "trigrams_st = get_top_ngrams(jobpostDF['description_cleaned_st'], (3, 3),20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gram before remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 12))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='Freq', y='Word', data=unigrams_st)\n",
    "plt.title('Top 20 Unigrams before removing stopwords', size=15)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.barplot(x='Freq', y='Word', data=bigrams_st)\n",
    "plt.title('Top 20 Bigrams before removing stopwords', size=15)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.barplot(x='Freq', y='Word', data=trigrams_st)\n",
    "plt.title('Top 20 Trigrams before removing stopwords', size=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gram before after Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 12))\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='Freq', y='Word', data=unigrams)\n",
    "plt.title('Top 20 Unigrams after removing stopwords', size=15)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.barplot(x='Freq', y='Word', data=bigrams)\n",
    "plt.title('Top 20 Bigrams after removing stopwords', size=15)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.barplot(x='Freq', y='Word', data=trigrams)\n",
    "plt.title('Top 20 Trigrams after removing stopwords', size=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gram Count in Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_converter = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, token_pattern=None)\n",
    "x = bow_converter.fit_transform(jobpostDF['description_cleaned'])\n",
    "words = bow_converter.get_feature_names_out()\n",
    "\n",
    "bigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=(2,2), lowercase=False, token_pattern=None) \n",
    "x2 = bigram_converter.fit_transform(jobpostDF['description_cleaned'])\n",
    "bigrams = bigram_converter.get_feature_names_out()\n",
    "\n",
    "trigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=(3,3), lowercase=False, token_pattern=None) \n",
    "x3 = trigram_converter.fit_transform(jobpostDF['description_cleaned'])\n",
    "trigrams = trigram_converter.get_feature_names_out()\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "counts = [len(words), len(bigrams), len(trigrams)]\n",
    "plt.plot(counts, color='blue')\n",
    "plt.plot(counts, 'bo')\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "plt.xticks(range(3), ['unigram', 'bigram', 'trigram'])\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.title('Number of ngrams in Job Description', {'fontsize':16})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with Job Skill Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobskillsDF = pd.read_csv('job_skills.csv')\n",
    "jobskillsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobskillsDF.isnull().sum()\n",
    "jobskillsDF = jobskillsDF.dropna(subset='skill_abr')\n",
    "jobskillsDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a Closer Look for How Many 'Job Skill' In This Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x=jobskillsDF['skill_abr'], width=0.6)\n",
    "\n",
    "palette = sns.color_palette(\"deep\", len(ax.patches))\n",
    "for bar, color in zip(ax.patches, palette):\n",
    "    bar.set_color(color)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=8, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "\n",
    "unique_skills = jobskillsDF['skill_abr'].nunique()\n",
    "ax.text(0.95, 0.95, f'Unique Skills : {unique_skills}', transform=ax.transAxes, \n",
    "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
    "\n",
    "unique_jobs = jobskillsDF['job_id'].nunique()\n",
    "ax.text(0.95, 0.85, f'Jobs : {unique_jobs}', transform=ax.transAxes, \n",
    "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
    "\n",
    "plt.title('Count of Job Skills', fontsize=16)\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "ax.set_xlabel(\"Skills\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regroup with Related Skill to Only 10 Skill + 1 Other Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jxWqt-E5Q_0"
   },
   "outputs": [],
   "source": [
    "#Grouping Skill to 10 Skill \n",
    "skill_mapping = { 'ADM': 'ADM', #1.Administration\n",
    "                 'CNSL': 'ADM',\n",
    "                  'HR': 'ADM',\n",
    "                  'LGL': 'ADM',\n",
    "                  'MGMT': 'ADM',\n",
    "                  'PRJM':'ADM',\n",
    "                  'ACCT':'FIN', #2.Business and Finace\n",
    "                  'CUST':'FIN',\n",
    "                  'DIST':'FIN',\n",
    "                  'FIN':'FIN',\n",
    "                  'PRCH':'FIN',\n",
    "                  'SALE':'FIN',\n",
    "                  'STRA':'FIN',\n",
    "                  'SUPL':'FIN',\n",
    "                  'BD':'FIN',\n",
    "                  'GENB':'FIN',\n",
    "                  'ART':'DSGN', #3.Creative and Design\n",
    "                  'DSGN':'DSGN',\n",
    "                  'WRT':'DSGN',\n",
    "                  'EDU':'EDU', #4.Education\n",
    "                  'TRNG':'EDU',\n",
    "                  'ENG':'ENG',#5.Engineering\n",
    "                  'IT':'ENG',\n",
    "                  'MNFC':'ENG',\n",
    "                  'HCPR':'HCPR',#6.Healthcare\n",
    "                  'ADVR':'MRKT',#7.Marketing and Advertising\n",
    "                  'MRKT':'MRKT',\n",
    "                  'PR':'MRKT',\n",
    "                  'PRDM':'PRDM',#8.Product Development\n",
    "                  'ANLS':'RSCH',#9.Research and Science\n",
    "                  'SCI':'RSCH',\n",
    "                  'RSCH':'RSCH',\n",
    "                  'QA':'RSCH',\n",
    "                  'PROD':'PROD'}#10.Project Management\n",
    "jobskillsDF['skill_abr_regroup'] = jobskillsDF['skill_abr'].replace(skill_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate row because 1 job may have multi skill but after grouping skill it can be the same skill\n",
    "jobskillsDF = jobskillsDF.drop_duplicates(subset=['job_id', 'skill_abr_regroup'])\n",
    "print(jobskillsDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Skills after Grouping\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x=jobskillsDF['skill_abr_regroup'], width=0.6)\n",
    "\n",
    "palette = sns.color_palette(\"deep\", len(ax.patches))\n",
    "for bar, color in zip(ax.patches, palette):\n",
    "    bar.set_color(color)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=8, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "\n",
    "unique_skills_regroup = jobskillsDF['skill_abr_regroup'].nunique()\n",
    "ax.text(0.95, 0.95, f'Unique Skills : {unique_skills_regroup}', transform=ax.transAxes, \n",
    "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
    "\n",
    "unique_jobs_regroup = jobskillsDF['job_id'].nunique()\n",
    "ax.text(0.95, 0.85, f'Jobs : {unique_jobs}', transform=ax.transAxes, \n",
    "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
    "\n",
    "plt.title('Count of Job Skills', fontsize=16)\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "ax.set_xlabel(\"Skills\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8pZ3suRPrgX"
   },
   "source": [
    "# Job Description VS Job Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Multi-Label for Job Skill (1 Job : Multi Skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsgrdHsxPpUh"
   },
   "outputs": [],
   "source": [
    "multiskillDF = pd.merge(jobpostDF, jobskillsDF, on='job_id', how='inner') # merge skill type with description mathc by job id\n",
    "multiskillDF = multiskillDF.groupby('job_id').agg({'title': 'first', 'description_cleaned': 'first', 'skill_abr_regroup': ','.join}).reset_index()\n",
    "multiskillDF['skill_count'] = multiskillDF['skill_abr_regroup'].str.split(',').apply(len)\n",
    "multiskillDF['skill_abr_regroup'] = multiskillDF['skill_abr_regroup'].str.split(',').tolist()\n",
    "multiskillDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x=multiskillDF['skill_count'], width=0.6)\n",
    "\n",
    "palette = sns.color_palette(\"pastel\", len(ax.patches))\n",
    "for bar, color in zip(ax.patches, palette):\n",
    "    bar.set_color(color)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=14, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "\n",
    "labels = [f\"{int(label) + 1} Skills\" for label in ax.get_xticks()]\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "plt.title('Count of Job Skills', fontsize=16)\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "ax.set_xlabel(\"Number of Skills required\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creat Single Label for Job Skill Dataframe to Meansure TF-IDF Score of Each Job Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneskillDF = multiskillDF[multiskillDF['skill_count'] == 1].copy()\n",
    "oneskillDF['skill_abr_regroup'] = oneskillDF['skill_abr_regroup'].str.join('')\n",
    "print(oneskillDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x=oneskillDF['skill_abr_regroup'], width=0.6)\n",
    "\n",
    "palette = sns.color_palette(\"deep\", len(ax.patches))\n",
    "for bar, color in zip(ax.patches, palette):\n",
    "    bar.set_color(color)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "\n",
    "nu_skill = oneskillDF['skill_abr_regroup'].nunique()\n",
    "ax.text(0.95, 0.95, f'Unique Skills : {nu_skill}', transform=ax.transAxes, \n",
    "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
    "\n",
    "jobs = oneskillDF['job_id'].nunique()\n",
    "ax.text(0.95, 0.85, f'Jobs : {jobs}', transform=ax.transAxes, \n",
    "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
    "\n",
    "plt.title('Count of Job Skills', fontsize=16)\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "ax.set_xlabel(\"Skills\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Score of Job Description and Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizor = TfidfVectorizer(min_df=5, \n",
    "                             max_df=0.5,\n",
    "                             analyzer='word',\n",
    "                             strip_accents='unicode',\n",
    "                             ngram_range=(1, 3),\n",
    "                             sublinear_tf=True, \n",
    "                             smooth_idf=True,\n",
    "                             use_idf=True)\n",
    "def top_tfidf_feats(row, features, top_n=20):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n=20):\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=10):\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=16):\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs\n",
    "\n",
    "def plot_tfidf_classfeats_h(dfs, num_class=9):\n",
    "    num_class = len(dfs)\n",
    "    fig = plt.figure(figsize=(12, num_class*10), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(num_class, 1, i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=16)\n",
    "        ax.set_ylabel(\"Word\", labelpad=16, fontsize=16)\n",
    "        ax.set_title(str(df.label) + ' Skill', fontsize=25)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.tfidf, align='center')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        ax.invert_yaxis()\n",
    "        yticks = ax.set_yticklabels(df.feature)\n",
    "        \n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "                tick.label1.set_fontsize(20) \n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.show()\n",
    "\n",
    "tfidf_vectorizor.fit(list(oneskillDF['description_cleaned']))\n",
    "\n",
    "class_Xtr = tfidf_vectorizor.transform(oneskillDF['description_cleaned'])\n",
    "class_y = oneskillDF['skill_abr_regroup']\n",
    "class_features = tfidf_vectorizor.get_feature_names_out()\n",
    "class_top_dfs = top_feats_by_class(class_Xtr, class_y, class_features)\n",
    "plot_tfidf_classfeats_h(class_top_dfs, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eyC8qaYfVyJ"
   },
   "source": [
    "# Train and Test data set and applying N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1, 2))\n",
    "transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1JjnIiFfWXc"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(multiskillDF['description_cleaned'],multiskillDF['skill_abr_regroup'], test_size = 0.20, random_state = 40)\n",
    "\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_test = mlb.transform(y_test)\n",
    "\n",
    "x_train_counts = count_vect.fit_transform(x_train)\n",
    "x_train_tfidf = transformer.fit_transform(x_train_counts)\n",
    "\n",
    "x_test_counts = count_vect.transform(x_test)\n",
    "x_test_tfidf = transformer.transform(x_test_counts)\n",
    "\n",
    "\n",
    "print (x_train_tfidf.shape,x_test_tfidf.shape, y_train.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and evaluating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrzZ4RUgYg3I"
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "001xiSCyUiVi"
   },
   "outputs": [],
   "source": [
    "#Count Vectorization\n",
    "lr = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "lr.fit(x_train_counts, y_train)\n",
    "y_pred1 = lr.predict(x_test_counts)\n",
    "print(\"Accuracy for Logistic Regression with Count Vectorization (Bag of Word): {:.2f}%\".format(accuracy_score(y_test, y_pred1)*100))\n",
    "print(classification_report(y_test, y_pred1, target_names=list(mlb.classes_)))\n",
    "\n",
    "#TF-IDF\n",
    "lr_tfidf = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "lr_tfidf.fit(x_train_tfidf, y_train)\n",
    "y_pred2 = lr_tfidf.predict(x_test_tfidf)\n",
    "print(\"Accuracy for Logistic Regression with TF-IDF: {:.2f}%\".format(accuracy_score(y_test, y_pred2)*100))\n",
    "print(classification_report(y_test, y_pred2, target_names=list(mlb.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wTUqJCgYg3I"
   },
   "source": [
    "Naive Bayes(Multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFfaGwGoYg3I"
   },
   "outputs": [],
   "source": [
    "#Count Vectorization\n",
    "mnb = MultiOutputClassifier(MultinomialNB())\n",
    "mnb.fit(x_train_counts, y_train)\n",
    "y_pred3 = mnb.predict(x_test_counts)\n",
    "print(\"Accuracy for Navie Bayes with Count Vectorization (Bag of Word): {:.2f}%\".format(accuracy_score(y_test, y_pred3)*100))\n",
    "print(classification_report(y_test, y_pred3, target_names=list(mlb.classes_)))\n",
    "\n",
    "#TF-IDF\n",
    "mnb_tfidf = MultiOutputClassifier(MultinomialNB())\n",
    "mnb_tfidf.fit(x_train_tfidf, y_train)\n",
    "y_pred4 = mnb_tfidf.predict(x_test_tfidf)\n",
    "print(\"Accuracy for Navie Bayes with TF-IDF: {:.2f}%\".format(accuracy_score(y_test, y_pred4)*100))\n",
    "print(classification_report(y_test, y_pred4, target_names=list(mlb.classes_)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Hyperparameter for ***Navie Bayes with TF-IDF method***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model = MultiOutputClassifier(MultinomialNB())\n",
    "alpha = [0.1,0.3,0.5]\n",
    "paramgrid = {'estimator__alpha':alpha}\n",
    "gsearch_cv = GridSearchCV(mnb_model, param_grid=paramgrid, cv=5)\n",
    "gsearch_cv.fit(x_train_tfidf, y_train)\n",
    "\n",
    "best_alpha = gsearch_cv.best_params_['estimator__alpha']\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "\n",
    "mean_test_scores = gsearch_cv.cv_results_['mean_test_score']\n",
    "plt.plot(alpha, mean_test_scores, marker='o')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean Test Score (Accuracy)')\n",
    "plt.title('Alpha vs. Mean Test Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Tuning HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_classify(X_tr, y_tr, X_test, y_test, description, multilabel=True):\n",
    "    if multilabel:\n",
    "        model = MultiOutputClassifier(LogisticRegression(max_iter=1000)).fit(X_tr, y_tr)\n",
    "        labeltype = 'Multi-Label'\n",
    "    else:\n",
    "        model = MultiOutputClassifier(LogisticRegression(max_iter=500)).fit(X_tr, y_tr)\n",
    "        labeltype = 'Single-Label'\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    #Classification Report\n",
    "    clf_report = classification_report(y_test, y_pred, target_names=list(mlb.classes_), output_dict=True, zero_division=1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)\n",
    "    plt.title(\"{} Logistic Regression Classification ({}) Accuracy Rate: {:.2f}%\".format(labeltype,description, accuracy_score(y_test, y_pred)*100))\n",
    "    plt.show()\n",
    "    #Confusesion Matrix (MultiLabel)\n",
    "    if multilabel:\n",
    "        n_labels = y_test.shape[1]\n",
    "        fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 12))\n",
    "        fig.suptitle('{} Logistic Regression Classification ({}) Confusion Matrix'.format(labeltype,description), fontsize=16)\n",
    "        \n",
    "        for idx, ax in enumerate(axes.ravel()):\n",
    "            if idx < n_labels:\n",
    "                cm = confusion_matrix(y_test[:, idx], y_pred[:, idx])\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "                disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "                ax.set_title(f'Label: {mlb.classes_[idx]}')\n",
    "            else:\n",
    "                ax.axis('off') \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.90)\n",
    "        plt.show()\n",
    "    #Confusesion Matrix (Single-Label)\n",
    "    else:\n",
    "        y_test_labels = y_test.argmax(axis=1)\n",
    "        y_pred_labels = y_pred.argmax(axis=1)\n",
    "        \n",
    "        cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=mlb.classes_)\n",
    "        disp.plot(cmap=plt.cm.Blues,values_format=\".2f\")\n",
    "        plt.title('{} Logistic Regression Classification ({}) Confusion Matrix'.format(labeltype,description))\n",
    "        \n",
    "        plt.xticks(fontsize=8)\n",
    "        plt.yticks(fontsize=8)\n",
    "        cbar = plt.gcf().axes[-1]\n",
    "        cbar.tick_params(labelsize=8)\n",
    "        \n",
    "        for text in disp.text_:\n",
    "            for t in text:\n",
    "                t.set_fontsize(9)\n",
    "                t.set_color('black')\n",
    "        \n",
    "        plt.show()\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_classify(X_tr, y_tr, X_test, y_test, description, multilabel=True):\n",
    "    if multilabel:\n",
    "        model = MultiOutputClassifier(MultinomialNB(alpha=0.1)).fit(X_tr, y_tr)\n",
    "        labeltype = 'Multi-Label'\n",
    "    else:\n",
    "        model = MultiOutputClassifier(MultinomialNB(alpha=0.1)).fit(X_tr, y_tr)\n",
    "        labeltype = 'Single-Label'\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    #Classification Report\n",
    "    clf_report = classification_report(y_test, y_pred, target_names=list(mlb.classes_), output_dict=True, zero_division=1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)\n",
    "    plt.title(\"{} NaiveBayes Classification ({}) Accuracy Rate: {:.2f}%\".format(labeltype,description, accuracy_score(y_test, y_pred)*100))\n",
    "    plt.show()\n",
    "    #Confusesion Matrix (MultiLabel)\n",
    "    if multilabel:\n",
    "        n_labels = y_test.shape[1]\n",
    "        fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 12))\n",
    "        fig.suptitle('{} NaiveBayes Classification ({}) Confusion Matrix'.format(labeltype,description), fontsize=16)\n",
    "        \n",
    "        for idx, ax in enumerate(axes.ravel()):\n",
    "            if idx < n_labels:\n",
    "                cm = confusion_matrix(y_test[:, idx], y_pred[:, idx])\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "                disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "                ax.set_title(f'Label: {mlb.classes_[idx]}')\n",
    "            else:\n",
    "                ax.axis('off') \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.90)\n",
    "        plt.show()\n",
    "    #Confusesion Matrix (Single-Label)\n",
    "    else:\n",
    "        y_test_labels = y_test.argmax(axis=1)\n",
    "        y_pred_labels = y_pred.argmax(axis=1)\n",
    "        \n",
    "        cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=mlb.classes_)\n",
    "        disp.plot(cmap=plt.cm.Blues,values_format=\".2f\")\n",
    "        plt.title('{} NaiveBayes Classification ({}) Confusion Matrix'.format(labeltype,description))\n",
    "        \n",
    "        plt.xticks(fontsize=8)\n",
    "        plt.yticks(fontsize=8)\n",
    "        cbar = plt.gcf().axes[-1]\n",
    "        cbar.tick_params(labelsize=8)\n",
    "        \n",
    "        for text in disp.text_:\n",
    "            for t in text:\n",
    "                t.set_fontsize(9)\n",
    "                t.set_color('black')\n",
    "        \n",
    "        plt.show()\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ​Multi Label - Skill Classificaiotn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_LR_model_bow = LR_classify(x_train_counts, y_train, x_test_counts, y_test, 'Bag Of Words',)\n",
    "multi_LR_model_tfidf = LR_classify(x_train_tfidf, y_train, x_test_tfidf, y_test, 'TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_NB_model_bow = NB_classify(x_train_counts, y_train, x_test_counts, y_test, 'Bag Of Words')\n",
    "multi_NB_model_tfidf = NB_classify(x_train_tfidf, y_train, x_test_tfidf, y_test, 'TF-IDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Skill Classification (One Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneskillDF['skill_abr_regroup'] = oneskillDF['skill_abr_regroup'].apply(lambda x: [x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1, 3))\n",
    "transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(oneskillDF['description_cleaned'],oneskillDF['skill_abr_regroup'], test_size = 0.20, random_state = 60)\n",
    "\n",
    "# Y_train = mlb.fit_transform(Y_train)\n",
    "# Y_test = mlb.transform(Y_test)\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_tfidf = transformer.fit_transform(X_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_tfidf = transformer.transform(X_test_counts)\n",
    "\n",
    "print (\"Bag of Word Shape :\",X_train_counts.shape,X_test_counts.shape, y_train.shape, y_test.shape)\n",
    "print (\"TF-IDF Shape :\",X_test_counts.shape,X_test_tfidf.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_ = MultinomialNB(alpha=0.1)\n",
    "mnb_.fit(X_train_counts, Y_train)\n",
    "y_pred_ = mnb_.predict(X_test_counts)\n",
    "print(\"Accuracy for Navie Bayes with Count Vectorization (Bag of Word): {:.2f}%\".format(accuracy_score(Y_test, y_pred_)*100))\n",
    "print(classification_report(Y_test, y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "one_LR_model_bow = LR_classify(X_train_counts, Y_train, X_test_counts, Y_test, 'Bag Of Words',multilabel=False)\n",
    "one_LR_model_tfidf = LR_classify(X_train_tfidf, Y_train, X_test_tfidf, Y_test, 'TF-IDF',multilabel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "one_NB_model_bow = NB_classify(X_train_counts, Y_train, X_test_counts, Y_test, 'Bag Of Words',multilabel=False)\n",
    "one_NB_model_tfidf = NB_classify(X_train_tfidf, Y_train, X_test_tfidf, Y_test, 'TF-IDF',multilabel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Skill Classification (One Label and Remove Unpredictable Skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decide to remove some skill that got low f1 score from previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutskillDF = oneskillDF[~oneskillDF['skill_abr_regroup'].isin([['PRDM'], ['PROD'], ['RSCH'],['DSGN'],['MRKT'],['EDU'],['OTHR']])].copy()\n",
    "print(cutskillDF.shape)\n",
    "cutskillDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1, 3))\n",
    "transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cXtrain, cXtest, cY_trian, cY_test = train_test_split(cutskillDF['description_cleaned'],cutskillDF['skill_abr_regroup'], test_size = 0.20, random_state = 60)\n",
    "\n",
    "cY_trian = mlb.fit_transform(cY_trian)\n",
    "cY_test = mlb.transform(cY_test)\n",
    "\n",
    "cXtrain_counts = count_vect.fit_transform(cXtrain)\n",
    "cXtrain_tfidf = transformer.fit_transform(cXtrain_counts)\n",
    "\n",
    "cXtest_counts = count_vect.transform(cXtest)\n",
    "cXtest_tfidf = transformer.transform(cXtest_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "cut_LR_model_bow = LR_classify(cXtrain_counts, cY_trian, cXtest_counts, cY_test, 'Bag Of Words',multilabel=False)\n",
    "cut_LR_model_tfidf = LR_classify(cXtrain_tfidf, cY_trian, cXtest_tfidf, cY_test, 'TF-IDF',multilabel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "cut_NB_model_bow = NB_classify(cXtrain_counts, cY_trian, cXtest_counts, cY_test, 'Bag Of Words',multilabel=False)\n",
    "cut_NB_model_tfidf = NB_classify(cXtrain_tfidf, cY_trian, cXtest_tfidf, cY_test, 'TF-IDF',multilabel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiskillDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(multiskillDF.skill_abr_regroup)\n",
    "Y = multilabel_binarizer.transform(multiskillDF.skill_abr_regroup)\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "X_counts = count_vect.fit_transform(multiskillDF.description_cleaned)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lp = LabelPowerset()\n",
    "Y_lp = lp.transform(Y)\n",
    "\n",
    "ros = RandomOverSampler(random_state=9000)\n",
    "X_tfidf_resampled, Y_tfidf_resampled_lp = ros.fit_resample(X_tfidf, Y_lp)\n",
    "\n",
    "Y_tfidf_resampled = lp.inverse_transform(Y_tfidf_resampled_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train_tfidf_resampled back to multilabel format\n",
    "y_train_tfidf_resampled_multilabel = multilabel_binarizer.inverse_transform(Y_tfidf_resampled)\n",
    "\n",
    "# Split the data\n",
    "x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf_resampled, y_train_tfidf_resampled_multilabel, test_size=0.2, random_state=9000)\n",
    "\n",
    "# Convert y_train_tfidf to binary matrix format\n",
    "y_train_tfidf_array = multilabel_binarizer.transform(y_train_tfidf)\n",
    "\n",
    "# Sum along axis=0 to get the class distribution\n",
    "y_train_tfidf_sum = y_train_tfidf_array.sum(axis=0)\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "(ax_test, ax_train) = fig.subplots(ncols=2, nrows=1)\n",
    "g1 = sns.barplot(x=Y.sum(axis=0), y=multilabel_binarizer.classes_, ax=ax_test)\n",
    "g2 = sns.barplot(x=y_train_tfidf_sum, y=multilabel_binarizer.classes_, ax=ax_train)\n",
    "g1.set_title(\"class distribution before resampling\")\n",
    "g2.set_title(\"class distribution in training set after resampling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_tfidf_array = multilabel_binarizer.transform(y_test_tfidf)\n",
    "clf = OneVsRestClassifier(MultinomialNB())\n",
    "# Train the classifier\n",
    "clf.fit(x_train_tfidf, y_train_tfidf_array)\n",
    "# Predict on the test set\n",
    "y_pred_tfidf = clf.predict(x_test_tfidf)\n",
    "print(\"Accuracy for Logistic Regression with TF-IDF: {:.2f}%\".format(accuracy_score(y_test_tfidf, y_pred_tfidf)*100))\n",
    "print(classification_report(y_test_tfidf, y_pred_tfidf, target_names=multilabel_binarizer.classes_))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3680745,
     "sourceId": 6893293,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
